{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:43:20.552746Z",
     "start_time": "2025-07-31T00:43:20.550420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ[\"RDKIT_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"]   = \"1\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "## or CUBLAS_WORKSPACE_CONFIG=:16:8\n",
    "\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)"
   ],
   "id": "22eb7c466cee29d4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-31T00:43:20.447975Z",
     "start_time": "2025-07-31T00:43:17.590119Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.distributions.beta import Beta\n",
    "import math\n",
    "from torch_geometric.nn import PNA, global_mean_pool\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import degree\n",
    "from torch_ema import ExponentialMovingAverage\n",
    "from rdkit import Chem\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "from graph_dataset_ogb import OnTheFlyOGBCompatibleSmilesDataset, mask_edges, mask_nodes"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:43:20.507125Z",
     "start_time": "2025-07-31T00:43:20.504892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalized_rmse(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return rmse / (np.max(y_true) - np.min(y_true))\n",
    "\n",
    "def pearson_correlation(y_true, y_pred):\n",
    "    corr = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "    return np.clip(corr, 0, 1)\n",
    "\n",
    "def competition_score(y_true, y_pred):\n",
    "    nrmse = min(normalized_rmse(y_true, y_pred), 1)\n",
    "    pearson = pearson_correlation(y_true, y_pred)\n",
    "    return 0.5 * (1 - nrmse) + 0.5 * pearson"
   ],
   "id": "cf22825c6bb2e93f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:43:35.554894Z",
     "start_time": "2025-07-31T00:43:35.552399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CFG = {\n",
    "    'EPOCHS': 200,\n",
    "    'PATIENCE': 25,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'WEIGHT_DECAY': 1e-4,\n",
    "    'BATCH_SIZE': 512,\n",
    "    'AUG_PROB': 0.5,\n",
    "    'SEED': 2025, # 6174\n",
    "    'NUM_WORKERS': 10,\n",
    "    'USE_AMP': False,\n",
    "}"
   ],
   "id": "299475069cff6326",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:43:36.176512Z",
     "start_time": "2025-07-31T00:43:36.172601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "seed_everything(CFG['SEED'])"
   ],
   "id": "e8ec6c193b990967",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:43:36.614893Z",
     "start_time": "2025-07-31T00:43:36.610091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# [기존 코드] MASK_NODE_FEATURE_VECTOR는 그대로 사용합니다.\n",
    "ATOM_FEATURE_VOCAB_SIZES = [119, 4, 12, 12, 10, 6, 6, 2, 2]\n",
    "MASK_NODE_FEATURE_VECTOR = torch.tensor(ATOM_FEATURE_VOCAB_SIZES, dtype=torch.long)\n",
    "\n",
    "class MaskNodes(object):\n",
    "    \"\"\"\n",
    "    하나의 그래프(Data 객체)에 대해 노드 마스킹을 적용하는 transform.\n",
    "    \"\"\"\n",
    "    def __init__(self, mask_ratio: float):\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "    def __call__(self, data: Data) -> Data:\n",
    "        if self.mask_ratio == 0.0:\n",
    "            return data\n",
    "\n",
    "        num_nodes = data.num_nodes\n",
    "        num_to_mask = int(num_nodes * self.mask_ratio)\n",
    "\n",
    "        if num_to_mask == 0:\n",
    "            return data\n",
    "\n",
    "        # 원본 데이터를 수정하지 않도록 복제\n",
    "        data = data.clone()\n",
    "\n",
    "        # 마스킹할 노드 인덱스 선택\n",
    "        perm = torch.randperm(num_nodes)\n",
    "        masked_node_indices = perm[:num_to_mask]\n",
    "\n",
    "        # [MASK] 토큰으로 교체\n",
    "        data.x[masked_node_indices] = MASK_NODE_FEATURE_VECTOR\n",
    "        return data\n",
    "\n",
    "class MaskEdges(object):\n",
    "    \"\"\"\n",
    "    하나의 그래프(Data 객체)에 대해 엣지 마스킹을 적용하는 transform.\n",
    "    \"\"\"\n",
    "    def __init__(self, mask_ratio: float):\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "    def __call__(self, data: Data) -> Data:\n",
    "        if self.mask_ratio == 0.0:\n",
    "            return data\n",
    "\n",
    "        num_edges = data.num_edges\n",
    "        if num_edges == 0:\n",
    "            return data\n",
    "\n",
    "        # 가정: 엣지는 (i,j)와 (j,i) 쌍으로 존재\n",
    "        num_edge_pairs = num_edges // 2\n",
    "        num_pairs_to_remove = int(num_edge_pairs * self.mask_ratio)\n",
    "\n",
    "        if num_pairs_to_remove == 0:\n",
    "            return data\n",
    "\n",
    "        # 원본 데이터를 수정하지 않도록 복제\n",
    "        data = data.clone()\n",
    "\n",
    "        # 제거할 엣지 쌍 인덱스 선택\n",
    "        perm = torch.randperm(num_edge_pairs)\n",
    "        pairs_to_remove_idx = perm[:num_pairs_to_remove]\n",
    "\n",
    "        reverse_pairs_to_remove_idx = pairs_to_remove_idx + num_edge_pairs\n",
    "        indices_to_remove = torch.cat([pairs_to_remove_idx, reverse_pairs_to_remove_idx])\n",
    "\n",
    "        keep_mask = torch.ones(num_edges, dtype=torch.bool)\n",
    "        keep_mask[indices_to_remove] = False\n",
    "\n",
    "        data.edge_index = data.edge_index[:, keep_mask]\n",
    "        if data.edge_attr is not None:\n",
    "            data.edge_attr = data.edge_attr[keep_mask]\n",
    "\n",
    "        return data\n",
    "\n",
    "class RandomApply(object):\n",
    "    \"\"\"\n",
    "    주어진 transform을 확률 p로 적용합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, transform, p: float):\n",
    "        self.transform = transform\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, data: Data) -> Data:\n",
    "        if random.random() < self.p:\n",
    "            return self.transform(data)\n",
    "        return data"
   ],
   "id": "5c2d287eb248a8dd",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:43:37.119824Z",
     "start_time": "2025-07-31T00:43:37.111674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cal_degree(dataset_):\n",
    "    loader_ = DataLoader(\n",
    "        dataset_, batch_size=128, shuffle=False,\n",
    "        num_workers=3, pin_memory=True, persistent_workers=True, prefetch_factor=1,\n",
    "        worker_init_fn=seed_worker\n",
    "    )\n",
    "\n",
    "    deg = torch.zeros(10, dtype=torch.long)\n",
    "    # for batch in tqdm(loader_, desc=\"차수 계산 중\"):\n",
    "    for batch in loader_:\n",
    "        d = degree(batch.edge_index[1], num_nodes=batch.num_nodes, dtype=torch.long)\n",
    "        deg += torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "    del loader_\n",
    "\n",
    "    print(f'차수 계산 완료, degree: {deg}')\n",
    "    return deg\n",
    "\n",
    "class ScaledSigmoid(nn.Module):\n",
    "    def __init__(self, scale=2.5):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(x * self.scale)\n",
    "\n",
    "class ClampedGeneralizedSigmoid(nn.Module):\n",
    "    def __init__(self, intermediate_range=(-50.0, 150.0), final_range=(0.0, 100.0)):\n",
    "        super().__init__()\n",
    "        self.inter_min, self.inter_max = intermediate_range\n",
    "        self.final_min, self.final_max = final_range\n",
    "        self.inter_range = self.inter_max - self.inter_min\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Generalized Sigmoid를 적용하여 중간 범위로 예측\n",
    "        sigmoid_x = torch.sigmoid(x)\n",
    "\n",
    "        final_pred = sigmoid_x * self.inter_range + self.inter_min\n",
    "\n",
    "        # 최종 범위로 예측값을 강제로 제한 (clamping)\n",
    "        final_pred = torch.clamp(final_pred, self.final_min, self.final_max)\n",
    "\n",
    "        return final_pred\n",
    "\n",
    "\n",
    "class pna4finetuning(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim: int, hidden_dim: int, num_layers: int, deg: torch.Tensor, edge_dim: int,\n",
    "            model_path=None, freeze_encoder: bool = False, scale_factor = 2.0,\n",
    "    ):\n",
    "        super(pna4finetuning, self).__init__()\n",
    "        self.activation = ScaledSigmoid(scale=scale_factor)\n",
    "        self.final_activation = ClampedGeneralizedSigmoid(\n",
    "            intermediate_range=(-50.0, 150.0), # 시그모이드의 목표 범위를 넓게 설정\n",
    "            final_range=(0.0, 100.0)           # 최종 출력 범위\n",
    "        )\n",
    "\n",
    "        aggregators = ['mean', 'min', 'max', 'std']\n",
    "        scalers = ['identity', 'amplification', 'attenuation']\n",
    "\n",
    "        self.encoder = PNA(\n",
    "            in_channels=input_dim,\n",
    "            hidden_channels=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            out_channels=None,\n",
    "            dropout=0.1,\n",
    "            act='relu',\n",
    "            # norm='BatchNorm',\n",
    "            norm=None,\n",
    "            jk='cat',\n",
    "            # PNA 필수 인자들\n",
    "            aggregators=aggregators,\n",
    "            scalers=scalers,\n",
    "            deg=deg,\n",
    "            edge_dim=edge_dim,\n",
    "        )\n",
    "\n",
    "        if model_path is not None:\n",
    "            # state_dict = torch.load(model_path, map_location='cpu')\n",
    "            # self.encoder.load_state_dict(state_dict)\n",
    "\n",
    "            pretrained_state_dict = torch.load(model_path, map_location='cpu')\n",
    "            encoder_state_dict = {}\n",
    "            for k, v in pretrained_state_dict.items():\n",
    "                if k.startswith(\"encoder.\"):\n",
    "                    # 'encoder.convs.0.weight' -> 'convs.0.weight'\n",
    "                    # replace의 세 번째 인자 1은 한 번만 바꾸도록 보장합니다.\n",
    "                    new_key = k.replace(\"encoder.\", \"\", 1)\n",
    "                    encoder_state_dict[new_key] = v\n",
    "\n",
    "            # 필터링 및 수정된 state_dict를 self.encoder에 로드합니다.\n",
    "            self.encoder.load_state_dict(encoder_state_dict)\n",
    "            print(\"가중치 로드 완료.\")\n",
    "\n",
    "        if freeze_encoder:\n",
    "            print(\"사전 학습된 인코더를 동결합니다 (학습되지 않음).\")\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            print(\"사전 학습된 인코더를 동결하지 않습니다 (전체 모델 학습).\")\n",
    "\n",
    "\n",
    "        self.pool = global_mean_pool\n",
    "\n",
    "        self.finetune_head = nn.Sequential(\n",
    "            # nn.LayerNorm(hidden_dim),\n",
    "\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            # nn.GELU(),\n",
    "\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "        )\n",
    "\n",
    "        # self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        node_features = batch.x.float()\n",
    "        edge_features = batch.edge_attr.float() if batch.edge_attr is not None else None\n",
    "\n",
    "        node_repr = self.encoder(\n",
    "            x=node_features,\n",
    "            edge_index=batch.edge_index,\n",
    "            edge_attr=edge_features,\n",
    "            batch=batch.batch\n",
    "        )\n",
    "\n",
    "        graph_repr = self.pool(node_repr, batch.batch)\n",
    "        out = self.finetune_head(graph_repr).squeeze(1)\n",
    "\n",
    "        # prediction = self.activation(out) * 100\n",
    "        prediction = torch.sigmoid(out) * 100\n",
    "        # prediction = (torch.tanh(out) + 1) * 50\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def encode_graph(self, batch):\n",
    "        node_features = batch.x.float()\n",
    "        edge_features = batch.edge_attr.float() if batch.edge_attr is not None else None\n",
    "\n",
    "        node_repr = self.encoder(\n",
    "            x=node_features,\n",
    "            edge_index=batch.edge_index,\n",
    "            edge_attr=edge_features,\n",
    "            batch=batch.batch\n",
    "        )\n",
    "        graph_repr = self.pool(node_repr, batch.batch)  # [B, hidden_dim]\n",
    "        return graph_repr\n",
    "\n",
    "    def predict_from_repr(self, graph_repr):\n",
    "        out = self.finetune_head(graph_repr).squeeze(1)\n",
    "        pred = torch.sigmoid(out) * 100\n",
    "        return pred\n",
    "\n",
    "    def forward_mixup(self, batch, lam, perm):\n",
    "        \"\"\"\n",
    "        batch: PyG Batch\n",
    "        lam: scalar tensor in [0,1]\n",
    "        perm: permutation indices over graphs in the batch (len == batch_size)\n",
    "        \"\"\"\n",
    "        repr_ = self.encode_graph(batch)                 # [B, H]\n",
    "        repr_perm = repr_[perm]                          # [B, H]\n",
    "\n",
    "        if lam.dim() == 0: # 스칼라\n",
    "            mixed = lam * repr_ + (1. - lam) * repr_perm\n",
    "        else: # 벡터\n",
    "            lam = lam.view(-1, 1)            # [B, 1]\n",
    "            mixed = lam * repr_ + (1. - lam) * repr_perm\n",
    "\n",
    "        pred = self.predict_from_repr(mixed)        # [B]\n",
    "        return pred"
   ],
   "id": "895e6089d7f2b6",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:43:37.527212Z",
     "start_time": "2025-07-31T00:43:37.524861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_smile_canonical(smile): # To avoid duplicates, for example: canonical '*C=C(*)C' == '*C(=C*)C'\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        canon_smile = Chem.MolToSmiles(mol, canonical=True)\n",
    "        # if smile != canon_smile:\n",
    "        #     print(f'{smile} > {canon_smile}')\n",
    "        return canon_smile\n",
    "    except:\n",
    "        return np.nan"
   ],
   "id": "19ba1c3ea23efead",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:43:39.957807Z",
     "start_time": "2025-07-31T00:43:38.522922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_name_list = ['AID_1851.csv', 'AID_884.csv', 'AID_885.csv']\n",
    "external_data_full = pd.DataFrame()\n",
    "\n",
    "for i in range(len(data_name_list)):\n",
    "    external_data_ = pd.read_csv(f'../external_data/{data_name_list[i]}')\n",
    "    external_data_full = pd.concat([external_data_full, external_data_], ignore_index=True)\n",
    "\n",
    "external_data_full = external_data_full.reset_index(drop=True)\n",
    "print(external_data_full.shape)\n",
    "external_data_full = external_data_full.drop_duplicates(subset=[\"Canonical_Smiles\"], keep=\"first\").reset_index(drop=True)\n",
    "print(external_data_full.shape)\n",
    "\n",
    "external_data_full[\"Canonical_Smiles\"] = external_data_full[\"Canonical_Smiles\"].map(make_smile_canonical)\n",
    "\n",
    "# external_data_full['Inhibition'] = external_data_full['Inhibition'].clip(lower=7.5)"
   ],
   "id": "4b2d8f0c6e539dd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29179, 5)\n",
      "(14421, 5)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:43:39.977858Z",
     "start_time": "2025-07-31T00:43:39.969666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "external_data_full['Inhibition'] = external_data_full['Inhibition_max'].clip(lower=0.0, upper=100.0)\n",
    "external_data_full"
   ],
   "id": "b870f302cd4ad621",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                        Canonical_Smiles  Inhibition_hill  \\\n",
       "0                 CCCC(=O)Nc1ccc(N2CCN(CC)CC2)c(Cl)c1.Cl         0.000000   \n",
       "1      O=c1[nH]c2cc3c(cc2cc1CN(CCCO)Cc1nnnn1Cc1ccc(F)...        75.434847   \n",
       "2             CC(=O)N(c1ccc2oc(=O)sc2c1)S(=O)(=O)c1cccs1        47.039728   \n",
       "3        COc1ccccc1C(c1nnnn1C(C)(C)C)N1CCN(Cc2ccncc2)CC1        85.388103   \n",
       "4          CC(=O)Nc1cccc(NC(=O)C2CCCN2C(=O)Nc2ccccc2C)c1         0.000000   \n",
       "...                                                  ...              ...   \n",
       "14416               N#CC(C#N)=Cc1ccc([N+](=O)[O-])c(O)c1        44.867806   \n",
       "14417  COC(=O)[C@@]1(Cc2ccc(F)cc2)[C@H]2c3cc(C(=O)N4C...        55.181273   \n",
       "14418  CC(=O)O[C@H]1C(=O)[C@]2(C)[C@H]([C@H](OC(=O)c3...        40.557578   \n",
       "14419            CC(C)(C)c1ccc(NS(=O)(=O)c2ccc(N)cc2)cc1        49.942458   \n",
       "14420                COc1cc(/C=C/C(=O)c2cccs2)cc(OC)c1OC        43.364314   \n",
       "\n",
       "       Inhibition_fit  Inhibition_max  Inhibition_eff  Inhibition  \n",
       "0                 NaN          2.2935             NaN      2.2935  \n",
       "1            105.4660         97.4718             NaN     97.4718  \n",
       "2            113.7150         94.7622             NaN     94.7622  \n",
       "3             96.2811         97.0468             NaN     97.0468  \n",
       "4                 NaN         -0.0054             NaN      0.0000  \n",
       "...               ...             ...             ...         ...  \n",
       "14416         44.9374         51.3134         36.8698     51.3134  \n",
       "14417         57.8365         58.4264         51.2518     58.4264  \n",
       "14418         40.5823         52.7098         42.5540     52.7098  \n",
       "14419         50.2667         50.2546         55.2667     50.2546  \n",
       "14420         46.6233         48.7562         53.6233     48.7562  \n",
       "\n",
       "[14421 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Canonical_Smiles</th>\n",
       "      <th>Inhibition_hill</th>\n",
       "      <th>Inhibition_fit</th>\n",
       "      <th>Inhibition_max</th>\n",
       "      <th>Inhibition_eff</th>\n",
       "      <th>Inhibition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCCC(=O)Nc1ccc(N2CCN(CC)CC2)c(Cl)c1.Cl</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.2935</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.2935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O=c1[nH]c2cc3c(cc2cc1CN(CCCO)Cc1nnnn1Cc1ccc(F)...</td>\n",
       "      <td>75.434847</td>\n",
       "      <td>105.4660</td>\n",
       "      <td>97.4718</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.4718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC(=O)N(c1ccc2oc(=O)sc2c1)S(=O)(=O)c1cccs1</td>\n",
       "      <td>47.039728</td>\n",
       "      <td>113.7150</td>\n",
       "      <td>94.7622</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94.7622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COc1ccccc1C(c1nnnn1C(C)(C)C)N1CCN(Cc2ccncc2)CC1</td>\n",
       "      <td>85.388103</td>\n",
       "      <td>96.2811</td>\n",
       "      <td>97.0468</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.0468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC(=O)Nc1cccc(NC(=O)C2CCCN2C(=O)Nc2ccccc2C)c1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0054</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14416</th>\n",
       "      <td>N#CC(C#N)=Cc1ccc([N+](=O)[O-])c(O)c1</td>\n",
       "      <td>44.867806</td>\n",
       "      <td>44.9374</td>\n",
       "      <td>51.3134</td>\n",
       "      <td>36.8698</td>\n",
       "      <td>51.3134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14417</th>\n",
       "      <td>COC(=O)[C@@]1(Cc2ccc(F)cc2)[C@H]2c3cc(C(=O)N4C...</td>\n",
       "      <td>55.181273</td>\n",
       "      <td>57.8365</td>\n",
       "      <td>58.4264</td>\n",
       "      <td>51.2518</td>\n",
       "      <td>58.4264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14418</th>\n",
       "      <td>CC(=O)O[C@H]1C(=O)[C@]2(C)[C@H]([C@H](OC(=O)c3...</td>\n",
       "      <td>40.557578</td>\n",
       "      <td>40.5823</td>\n",
       "      <td>52.7098</td>\n",
       "      <td>42.5540</td>\n",
       "      <td>52.7098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14419</th>\n",
       "      <td>CC(C)(C)c1ccc(NS(=O)(=O)c2ccc(N)cc2)cc1</td>\n",
       "      <td>49.942458</td>\n",
       "      <td>50.2667</td>\n",
       "      <td>50.2546</td>\n",
       "      <td>55.2667</td>\n",
       "      <td>50.2546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14420</th>\n",
       "      <td>COc1cc(/C=C/C(=O)c2cccs2)cc(OC)c1OC</td>\n",
       "      <td>43.364314</td>\n",
       "      <td>46.6233</td>\n",
       "      <td>48.7562</td>\n",
       "      <td>53.6233</td>\n",
       "      <td>48.7562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14421 rows × 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:43:40.223058Z",
     "start_time": "2025-07-31T00:43:40.036480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv('../data/train.csv')\n",
    "data[\"Canonical_Smiles\"] = data[\"Canonical_Smiles\"].map(make_smile_canonical)\n",
    "data['Inhibition_binary'] = (data['Inhibition'] > 0.1).astype(int)\n",
    "data['Inhibition_bin10'] = pd.qcut(data['Inhibition'], q=10, labels=False, duplicates='drop').astype(int)\n",
    "\n",
    "print(data.shape)"
   ],
   "id": "26caa77a09d3f25e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1681, 5)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:43:40.454330Z",
     "start_time": "2025-07-31T00:43:40.448904Z"
    }
   },
   "cell_type": "code",
   "source": "data",
   "id": "1af3ae8321797f1a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              ID                                   Canonical_Smiles  \\\n",
       "0     TRAIN_0000                        Cl.OC1(Cc2cccc(Br)c2)CCNCC1   \n",
       "1     TRAIN_0001                            Brc1ccc2c3c(ccnc13)CCO2   \n",
       "2     TRAIN_0002         CC1(CO)CC(c2cc([N+](=O)[O-])c(F)cc2Cl)=NO1   \n",
       "3     TRAIN_0003  O=C(c1cccc(OCc2cccc(Nc3nc4ccc(F)cc4[nH]3)c2)c1...   \n",
       "4     TRAIN_0004     CC(C)CC(=O)c1c(S(C)=O)[nH]c2c(Cl)ccc(Cl)c2c1=O   \n",
       "...          ...                                                ...   \n",
       "1676  TRAIN_1676  Cc1cc2ncn(CC3CCN(S(=O)(=O)CCN4C(=O)CCCC4=O)CC3...   \n",
       "1677  TRAIN_1677            O=C(Cn1ncccc1=O)N1Cc2cnc(N3CCOCC3)nc2C1   \n",
       "1678  TRAIN_1678                 COc1coc(C(=O)Nc2cccc3c2ccn3C)cc1=O   \n",
       "1679  TRAIN_1679         Cc1cc(=O)n(CCNC(=O)c2nc3nc(C)cc(C)n3n2)cn1   \n",
       "1680  TRAIN_1680              CCc1ccc(/C=N/Nc2nn3cnnc3c3ccccc23)cc1   \n",
       "\n",
       "      Inhibition  Inhibition_binary  Inhibition_bin10  \n",
       "0      12.500000                  1                 2  \n",
       "1       4.450000                  1                 1  \n",
       "2       4.920000                  1                 1  \n",
       "3      71.500000                  1                 8  \n",
       "4      18.300000                  1                 3  \n",
       "...          ...                ...               ...  \n",
       "1676    0.500000                  1                 0  \n",
       "1677    0.500000                  1                 0  \n",
       "1678    0.500000                  1                 0  \n",
       "1679    0.500000                  1                 0  \n",
       "1680   41.700398                  1                 6  \n",
       "\n",
       "[1681 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Canonical_Smiles</th>\n",
       "      <th>Inhibition</th>\n",
       "      <th>Inhibition_binary</th>\n",
       "      <th>Inhibition_bin10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>Cl.OC1(Cc2cccc(Br)c2)CCNCC1</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>Brc1ccc2c3c(ccnc13)CCO2</td>\n",
       "      <td>4.450000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>CC1(CO)CC(c2cc([N+](=O)[O-])c(F)cc2Cl)=NO1</td>\n",
       "      <td>4.920000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>O=C(c1cccc(OCc2cccc(Nc3nc4ccc(F)cc4[nH]3)c2)c1...</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>CC(C)CC(=O)c1c(S(C)=O)[nH]c2c(Cl)ccc(Cl)c2c1=O</td>\n",
       "      <td>18.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1676</th>\n",
       "      <td>TRAIN_1676</td>\n",
       "      <td>Cc1cc2ncn(CC3CCN(S(=O)(=O)CCN4C(=O)CCCC4=O)CC3...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>TRAIN_1677</td>\n",
       "      <td>O=C(Cn1ncccc1=O)N1Cc2cnc(N3CCOCC3)nc2C1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678</th>\n",
       "      <td>TRAIN_1678</td>\n",
       "      <td>COc1coc(C(=O)Nc2cccc3c2ccn3C)cc1=O</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679</th>\n",
       "      <td>TRAIN_1679</td>\n",
       "      <td>Cc1cc(=O)n(CCNC(=O)c2nc3nc(C)cc(C)n3n2)cn1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>TRAIN_1680</td>\n",
       "      <td>CCc1ccc(/C=N/Nc2nn3cnnc3c3ccccc23)cc1</td>\n",
       "      <td>41.700398</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1681 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:43:41.401447Z",
     "start_time": "2025-07-31T00:43:40.958905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')"
   ],
   "id": "8ee1a204fda7f9c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:43:41.905833Z",
     "start_time": "2025-07-31T00:43:41.897284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_fn(\n",
    "        model, loader, optimizer, criterion, ema,\n",
    "        mixup_alpha=0.4, mixup_prob=1.0, mixup_off_epoch=None, epoch_idx=0):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    do_mixup_now = (mixup_prob > 0.0) and (mixup_off_epoch is None or epoch_idx < mixup_off_epoch)\n",
    "\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(DEVICE)\n",
    "        y = batch.y.view(-1).to(torch.float32)\n",
    "        B = y.shape[0]\n",
    "\n",
    "        if do_mixup_now and B > 1:\n",
    "            # 샘플별 마스크 m ~ Bernoulli(mixup_prob)\n",
    "            m = (torch.rand(B, device=DEVICE) < mixup_prob).to(torch.float32)  # [B]\n",
    "\n",
    "            # 샘플별 λ (벡터) — 원하면 스칼라 한 개만 써도 됨\n",
    "            lam_vec = Beta(mixup_alpha, mixup_alpha).sample((B,)).to(DEVICE)   # [B]\n",
    "            # (옵션) 대칭화: 충분히 섞이도록\n",
    "            lam_vec = torch.maximum(lam_vec, 1.0 - lam_vec)\n",
    "\n",
    "            # 선택되지 않은 샘플은 lam_eff=1 → 원본 유지\n",
    "            lam_eff = m * lam_vec + (1.0 - m) * 1.0                              # [B]\n",
    "\n",
    "            perm = torch.randperm(B, device=DEVICE)\n",
    "            outputs = model.forward_mixup(batch, lam_eff, perm).view(-1)\n",
    "            y_perm = y[perm]\n",
    "            y_mix = lam_eff * y + (1.0 - lam_eff) * y_perm\n",
    "            loss = criterion(outputs, y_mix)\n",
    "        else:\n",
    "            outputs = model(batch).view(-1)\n",
    "            loss = criterion(outputs, y)\n",
    "\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 1000)\n",
    "        optimizer.step()\n",
    "        ema.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(loader)\n",
    "\n",
    "def valid_fn(model, loader, is_tta):\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            if is_tta:\n",
    "                mask_data1 = mask_nodes(copy.deepcopy(data), 0.1)\n",
    "                mask_data2 = mask_edges(copy.deepcopy(data), 0.1)\n",
    "\n",
    "                data = data.to(DEVICE)\n",
    "                mask_data1 = mask_data1.to(DEVICE)\n",
    "                mask_data2 = mask_data2.to(DEVICE)\n",
    "\n",
    "                # 원본 + 증강 예측\n",
    "                out_orig = model(data).view(-1)\n",
    "                out_node = model(mask_data1).view(-1)\n",
    "                out_edge = model(mask_data2).view(-1)\n",
    "\n",
    "                # 평균(TTA)\n",
    "                outputs = torch.stack([out_orig, out_node, out_edge], dim=0).mean(dim=0)\n",
    "            else:\n",
    "                data = data.to(DEVICE)\n",
    "                outputs = model(data).view(-1)\n",
    "\n",
    "            all_preds.append(outputs.detach().cpu().float().numpy())\n",
    "            all_labels.append(data.y.view(-1).detach().cpu().float().numpy())\n",
    "\n",
    "    final_preds = np.concatenate(all_preds)\n",
    "    final_labels = np.concatenate(all_labels)\n",
    "\n",
    "    nrmse = normalized_rmse(final_labels, final_preds)\n",
    "    pearson = pearson_correlation(final_labels, final_preds)\n",
    "    score = competition_score(final_labels, final_preds)\n",
    "\n",
    "    return nrmse, pearson, score\n",
    "\n",
    "def evaluate_with_bootstrap(model, loader, is_tta, n_repeats=30, sample_ratio=0.8):\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # 1. 먼저 전체 검증 데이터에 대한 예측을 한 번에 수행합니다.\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            if is_tta:\n",
    "                # 원본 데이터와 증강 데이터 준비\n",
    "                mask_data1 = mask_nodes(copy.deepcopy(data), 0.1)\n",
    "                mask_data2 = mask_edges(copy.deepcopy(data), 0.1)\n",
    "\n",
    "                data = data.to(DEVICE)\n",
    "                mask_data1 = mask_data1.to(DEVICE)\n",
    "                mask_data2 = mask_data2.to(DEVICE)\n",
    "\n",
    "                # 원본 + 증강 예측\n",
    "                out_orig = model(data).view(-1)\n",
    "                out_node = model(mask_data1).view(-1)\n",
    "                out_edge = model(mask_data2).view(-1)\n",
    "\n",
    "                # 평균(TTA)\n",
    "                outputs = torch.stack([out_orig, out_node, out_edge], dim=0).mean(dim=0)\n",
    "            else:\n",
    "                data = data.to(DEVICE)\n",
    "                outputs = model(data).view(-1)\n",
    "\n",
    "            all_preds.append(outputs.detach().cpu().float().numpy())\n",
    "            all_labels.append(data.y.view(-1).detach().cpu().float().numpy())\n",
    "\n",
    "    final_preds = np.concatenate(all_preds)\n",
    "    final_labels = np.concatenate(all_labels)\n",
    "\n",
    "    # 2. 부트스트랩(Bootstrap) 샘플링 및 점수 계산\n",
    "    bootstrap_scores = []\n",
    "    n_total = len(final_labels)\n",
    "    sample_size = int(n_total * sample_ratio)\n",
    "\n",
    "    for _ in range(n_repeats):\n",
    "        # 비복원추출로 전체 데이터의 80%에 해당하는 인덱스를 랜덤하게 선택\n",
    "        indices = np.random.choice(n_total, size=sample_size, replace=False)\n",
    "\n",
    "        # 해당 인덱스로 라벨과 예측값 샘플링\n",
    "        sampled_labels = final_labels[indices]\n",
    "        sampled_preds = final_preds[indices]\n",
    "\n",
    "        # 샘플링된 데이터로 점수 계산 후 리스트에 추가\n",
    "        score = competition_score(sampled_labels, sampled_preds)\n",
    "        bootstrap_scores.append(score)\n",
    "\n",
    "    # 3. 반복하여 얻은 점수들의 평균과 표준편차 계산\n",
    "    mean_score = np.mean(bootstrap_scores)\n",
    "    std_score = np.std(bootstrap_scores)\n",
    "\n",
    "    return mean_score, std_score\n",
    "\n",
    "\n",
    "\n",
    "def predict(model, loader, is_tta):\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            if is_tta:\n",
    "                mask_data1 = mask_nodes(copy.deepcopy(data), 0.1)\n",
    "                mask_data2 = mask_edges(copy.deepcopy(data), 0.1)\n",
    "\n",
    "                data = data.to(DEVICE)\n",
    "                mask_data1 = mask_data1.to(DEVICE)\n",
    "                mask_data2 = mask_data2.to(DEVICE)\n",
    "\n",
    "                # 원본 + 증강 예측\n",
    "                out_orig = model(data).view(-1)\n",
    "                out_node = model(mask_data1).view(-1)\n",
    "                out_edge = model(mask_data2).view(-1)\n",
    "\n",
    "                # 평균(TTA)\n",
    "                outputs = torch.stack([out_orig, out_node, out_edge], dim=0).mean(dim=0)\n",
    "\n",
    "            else:\n",
    "                data = data.to(DEVICE)\n",
    "                outputs = model(data).view(-1)\n",
    "\n",
    "            all_preds.append(outputs.detach().cpu().float().numpy())\n",
    "\n",
    "    final_preds = np.concatenate(all_preds)\n",
    "    return final_preds"
   ],
   "id": "f385bd2edf0433b",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:47:24.761136Z",
     "start_time": "2025-07-31T00:47:24.758701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "N_SPLITS = 3\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=CFG['SEED'])\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=CFG['SEED'])"
   ],
   "id": "9bebb7a54c04f3e7",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:57:21.462405Z",
     "start_time": "2025-07-31T00:47:26.316067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "external_train_list = []\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(external_data_full)):\n",
    "    external_train = external_data_full.iloc[tr_idx].reset_index(drop=True)\n",
    "\n",
    "    # external_train_list.append(external_train)\n",
    "\n",
    "    mask_zero = external_train['Inhibition'] == 0\n",
    "    zeros_df = external_train[mask_zero]\n",
    "    nonzeros_df = external_train[~mask_zero]\n",
    "\n",
    "    # 남길 0 샘플 개수 계산 (기존 데이터에서 5%정도 되었으므로 비슷하게 유지)\n",
    "    # zero_ratio = 0.05\n",
    "    zero_ratio = 0.058299\n",
    "    n_nonzero = len(nonzeros_df)\n",
    "    target_zero_count = int(round(zero_ratio * n_nonzero / (1 - zero_ratio)))\n",
    "\n",
    "    # 무작위 추출\n",
    "    keep_zero_n = min(target_zero_count, len(zeros_df))\n",
    "    sampled_zeros = zeros_df.sample(n=keep_zero_n, random_state=CFG[\"SEED\"])\n",
    "\n",
    "    external_sub = pd.concat([nonzeros_df, sampled_zeros], ignore_index=True).reset_index(drop=True)\n",
    "    external_train_list.append(external_sub)\n",
    "\n",
    "# for fold, (train_idx, valid_idx) in enumerate(skf.split(data, data['Inhibition_binary'])):\n",
    "# for fold, (train_idx, valid_idx) in enumerate(skf.split(data, data['Inhibition_bin10'])):\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(data)):\n",
    "    print(f\"\\n\\n========== Fold {fold+1}/{N_SPLITS} ==========\")\n",
    "    fold_seed = CFG['SEED'] + (fold + 1) * 999\n",
    "\n",
    "\n",
    "    fold_train = data.iloc[train_idx].copy().reset_index(drop=True)\n",
    "    fold_valid = data.iloc[valid_idx].copy().reset_index(drop=True)\n",
    "\n",
    "    fold_train = pd.concat([fold_train, external_train_list[fold]], ignore_index=True).reset_index(drop=True)\n",
    "    # fold_train = pd.concat([fold_train, external_data_full], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "    if fold == 0:\n",
    "        print(f'Train Size: {fold_train.shape[0]}, Valid Size: {fold_valid.shape[0]}')\n",
    "\n",
    "    train_transforms = T.Compose([\n",
    "        RandomApply(MaskNodes(mask_ratio=0.1), p=0.5),\n",
    "        RandomApply(MaskEdges(mask_ratio=0.1), p=0.5),\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        del train_dataset, train_loader\n",
    "        del valid_dataset, valid_loader\n",
    "        del model\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    train_dataset = OnTheFlyOGBCompatibleSmilesDataset(\n",
    "        root=\"../train_tmp/MySmilesDataset\",\n",
    "        smiles_list=fold_train['Canonical_Smiles'].to_list(),\n",
    "        labels_list=fold_train['Inhibition'].to_list(),\n",
    "        # transform=train_transforms,\n",
    "        transform=None,\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        batch_size=CFG['BATCH_SIZE'],\n",
    "        num_workers=CFG['NUM_WORKERS'],\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=torch.Generator().manual_seed(CFG['SEED']),\n",
    "    )\n",
    "\n",
    "    valid_dataset = OnTheFlyOGBCompatibleSmilesDataset(\n",
    "        root=\"../valid_tmp/MySmilesDataset\",\n",
    "        smiles_list=fold_valid['Canonical_Smiles'].to_list(),\n",
    "        labels_list=fold_valid['Inhibition'].to_list(),\n",
    "        transform=None,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=CFG['BATCH_SIZE'],\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "\n",
    "    INPUT_DIM = train_dataset.num_node_features # 9\n",
    "    EDGE_DIM = train_dataset.num_edge_features # 3\n",
    "    HIDDEN_DIM = 128\n",
    "    NUM_LAYERS = 5\n",
    "    deg = cal_degree(train_dataset)\n",
    "\n",
    "    model = pna4finetuning(\n",
    "        input_dim=INPUT_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        deg=deg.to(DEVICE),\n",
    "        edge_dim=EDGE_DIM,\n",
    "        model_path='model_weights/pna_128_5_epoch30.pt',\n",
    "        freeze_encoder=False,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    if fold == 0:\n",
    "        print(model)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    # criterion = CompetitionLoss(alpha=1.0, beta=1.0).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=CFG['LEARNING_RATE'], weight_decay=CFG['WEIGHT_DECAY'])\n",
    "    ema = ExponentialMovingAverage(model.parameters(), decay=0.995)\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)\n",
    "\n",
    "    patience = 0\n",
    "    best_nrmse = float('inf')\n",
    "    best_pearson = -float('inf')\n",
    "    best_score = -float('inf')\n",
    "    best_epoch = 0\n",
    "    best_model_weights = None\n",
    "    for epoch in range(CFG['EPOCHS']):\n",
    "        train_loss = train_fn(model, train_loader, optimizer, criterion, ema, mixup_alpha=0.4, mixup_prob=0.5,)\n",
    "        nrmse, pearson, score = valid_fn(model, valid_loader, is_tta=False)\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        scheduler.step()\n",
    "\n",
    "        if score > best_score:\n",
    "            best_nrmse = nrmse\n",
    "            best_pearson = pearson\n",
    "            best_score = score\n",
    "            best_epoch = epoch + 1\n",
    "            patience = 0\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience > CFG['PATIENCE']:\n",
    "                break\n",
    "\n",
    "        print(\n",
    "            f\"[Fold {fold+1} Epoch {epoch+1:02d} LR {current_lr:.2e}] \"\n",
    "            f\"Train loss {train_loss:.3f} | \"\n",
    "            f\"Valid NRMSE {nrmse:.4f}, Pearson {pearson:.3f}, Score {score:.4f} | \"\n",
    "            f\"Patience: {patience}\"\n",
    "        )\n",
    "\n",
    "    if best_model_weights is not None:\n",
    "        seed = CFG['SEED']\n",
    "        save_path = f'./model_weights/model_{seed}_{fold+1}.pt'\n",
    "        torch.save(best_model_weights, save_path)\n",
    "        print(f'model saved in {save_path}')\n",
    "\n",
    "    print(f'[Fold {fold+1}] Best epoch {best_epoch} | '\n",
    "          f'NRMSE: {best_nrmse:.4f} Pearson: {best_pearson:.4f} Score: {best_score:.4f}')\n"
   ],
   "id": "cb233c17366769ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========== Fold 1/3 ==========\n",
      "Train Size: 9748, Valid Size: 561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "차수 계산 완료, degree: tensor([   578,  42843, 116284,  71680,   4666,      1,      0,      0,      0,\n",
      "             0])\n",
      "가중치 로드 완료.\n",
      "사전 학습된 인코더를 동결하지 않습니다 (전체 모델 학습).\n",
      "pna4finetuning(\n",
      "  (activation): ScaledSigmoid()\n",
      "  (final_activation): ClampedGeneralizedSigmoid()\n",
      "  (encoder): PNA(9, 128, num_layers=5)\n",
      "  (finetune_head): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "[Fold 1 Epoch 01 LR 1.00e-04] Train loss 1386.388 | Valid NRMSE 0.3451, Pearson 0.252, Score 0.4535 | Patience: 0\n",
      "[Fold 1 Epoch 02 LR 9.76e-05] Train loss 1132.749 | Valid NRMSE 0.3441, Pearson 0.235, Score 0.4453 | Patience: 1\n",
      "[Fold 1 Epoch 03 LR 9.05e-05] Train loss 1036.701 | Valid NRMSE 0.3284, Pearson 0.279, Score 0.4751 | Patience: 0\n",
      "[Fold 1 Epoch 04 LR 7.94e-05] Train loss 971.891 | Valid NRMSE 0.3110, Pearson 0.292, Score 0.4908 | Patience: 0\n",
      "[Fold 1 Epoch 05 LR 6.55e-05] Train loss 889.092 | Valid NRMSE 0.3030, Pearson 0.349, Score 0.5232 | Patience: 0\n",
      "[Fold 1 Epoch 06 LR 5.00e-05] Train loss 855.054 | Valid NRMSE 0.2788, Pearson 0.332, Score 0.5264 | Patience: 0\n",
      "[Fold 1 Epoch 07 LR 3.45e-05] Train loss 830.676 | Valid NRMSE 0.2768, Pearson 0.335, Score 0.5292 | Patience: 0\n",
      "[Fold 1 Epoch 08 LR 2.06e-05] Train loss 802.336 | Valid NRMSE 0.2826, Pearson 0.359, Score 0.5381 | Patience: 0\n",
      "[Fold 1 Epoch 09 LR 9.55e-06] Train loss 788.425 | Valid NRMSE 0.2804, Pearson 0.354, Score 0.5366 | Patience: 1\n",
      "[Fold 1 Epoch 10 LR 2.45e-06] Train loss 765.788 | Valid NRMSE 0.2769, Pearson 0.354, Score 0.5384 | Patience: 0\n",
      "[Fold 1 Epoch 11 LR 1.00e-04] Train loss 783.447 | Valid NRMSE 0.2641, Pearson 0.311, Score 0.5236 | Patience: 1\n",
      "[Fold 1 Epoch 12 LR 9.76e-05] Train loss 750.972 | Valid NRMSE 0.2663, Pearson 0.341, Score 0.5375 | Patience: 2\n",
      "[Fold 1 Epoch 13 LR 9.05e-05] Train loss 731.080 | Valid NRMSE 0.2730, Pearson 0.349, Score 0.5378 | Patience: 3\n",
      "[Fold 1 Epoch 14 LR 7.94e-05] Train loss 711.697 | Valid NRMSE 0.2841, Pearson 0.374, Score 0.5452 | Patience: 0\n",
      "[Fold 1 Epoch 15 LR 6.55e-05] Train loss 685.075 | Valid NRMSE 0.2716, Pearson 0.344, Score 0.5362 | Patience: 1\n",
      "[Fold 1 Epoch 16 LR 5.00e-05] Train loss 676.637 | Valid NRMSE 0.2546, Pearson 0.373, Score 0.5591 | Patience: 0\n",
      "[Fold 1 Epoch 17 LR 3.45e-05] Train loss 660.930 | Valid NRMSE 0.2579, Pearson 0.385, Score 0.5635 | Patience: 0\n",
      "[Fold 1 Epoch 18 LR 2.06e-05] Train loss 666.048 | Valid NRMSE 0.2637, Pearson 0.393, Score 0.5647 | Patience: 0\n",
      "[Fold 1 Epoch 19 LR 9.55e-06] Train loss 681.105 | Valid NRMSE 0.2697, Pearson 0.382, Score 0.5560 | Patience: 1\n",
      "[Fold 1 Epoch 20 LR 2.45e-06] Train loss 660.714 | Valid NRMSE 0.2624, Pearson 0.387, Score 0.5621 | Patience: 2\n",
      "[Fold 1 Epoch 21 LR 1.00e-04] Train loss 665.013 | Valid NRMSE 0.2575, Pearson 0.373, Score 0.5577 | Patience: 3\n",
      "[Fold 1 Epoch 22 LR 9.76e-05] Train loss 663.281 | Valid NRMSE 0.2612, Pearson 0.373, Score 0.5557 | Patience: 4\n",
      "[Fold 1 Epoch 23 LR 9.05e-05] Train loss 658.412 | Valid NRMSE 0.2597, Pearson 0.366, Score 0.5532 | Patience: 5\n",
      "[Fold 1 Epoch 24 LR 7.94e-05] Train loss 647.564 | Valid NRMSE 0.2506, Pearson 0.380, Score 0.5646 | Patience: 6\n",
      "[Fold 1 Epoch 25 LR 6.55e-05] Train loss 644.950 | Valid NRMSE 0.2509, Pearson 0.430, Score 0.5895 | Patience: 0\n",
      "[Fold 1 Epoch 26 LR 5.00e-05] Train loss 609.143 | Valid NRMSE 0.2513, Pearson 0.387, Score 0.5680 | Patience: 1\n",
      "[Fold 1 Epoch 27 LR 3.45e-05] Train loss 602.264 | Valid NRMSE 0.2541, Pearson 0.401, Score 0.5736 | Patience: 2\n",
      "[Fold 1 Epoch 28 LR 2.06e-05] Train loss 600.979 | Valid NRMSE 0.2547, Pearson 0.407, Score 0.5764 | Patience: 3\n",
      "[Fold 1 Epoch 29 LR 9.55e-06] Train loss 581.614 | Valid NRMSE 0.2539, Pearson 0.404, Score 0.5752 | Patience: 4\n",
      "[Fold 1 Epoch 30 LR 2.45e-06] Train loss 606.638 | Valid NRMSE 0.2525, Pearson 0.407, Score 0.5772 | Patience: 5\n",
      "[Fold 1 Epoch 31 LR 1.00e-04] Train loss 614.390 | Valid NRMSE 0.2529, Pearson 0.372, Score 0.5597 | Patience: 6\n",
      "[Fold 1 Epoch 32 LR 9.76e-05] Train loss 595.479 | Valid NRMSE 0.2591, Pearson 0.373, Score 0.5570 | Patience: 7\n",
      "[Fold 1 Epoch 33 LR 9.05e-05] Train loss 598.225 | Valid NRMSE 0.2492, Pearson 0.418, Score 0.5844 | Patience: 8\n",
      "[Fold 1 Epoch 34 LR 7.94e-05] Train loss 583.218 | Valid NRMSE 0.2517, Pearson 0.427, Score 0.5877 | Patience: 9\n",
      "[Fold 1 Epoch 35 LR 6.55e-05] Train loss 595.512 | Valid NRMSE 0.2772, Pearson 0.385, Score 0.5541 | Patience: 10\n",
      "[Fold 1 Epoch 36 LR 5.00e-05] Train loss 576.172 | Valid NRMSE 0.2615, Pearson 0.416, Score 0.5772 | Patience: 11\n",
      "[Fold 1 Epoch 37 LR 3.45e-05] Train loss 563.011 | Valid NRMSE 0.2535, Pearson 0.412, Score 0.5793 | Patience: 12\n",
      "[Fold 1 Epoch 38 LR 2.06e-05] Train loss 558.851 | Valid NRMSE 0.2544, Pearson 0.424, Score 0.5848 | Patience: 13\n",
      "[Fold 1 Epoch 39 LR 9.55e-06] Train loss 559.688 | Valid NRMSE 0.2511, Pearson 0.421, Score 0.5850 | Patience: 14\n",
      "[Fold 1 Epoch 40 LR 2.45e-06] Train loss 552.051 | Valid NRMSE 0.2502, Pearson 0.420, Score 0.5849 | Patience: 15\n",
      "[Fold 1 Epoch 41 LR 1.00e-04] Train loss 565.129 | Valid NRMSE 0.2466, Pearson 0.429, Score 0.5911 | Patience: 0\n",
      "[Fold 1 Epoch 42 LR 9.76e-05] Train loss 550.738 | Valid NRMSE 0.2732, Pearson 0.393, Score 0.5598 | Patience: 1\n",
      "[Fold 1 Epoch 43 LR 9.05e-05] Train loss 550.386 | Valid NRMSE 0.2840, Pearson 0.401, Score 0.5585 | Patience: 2\n",
      "[Fold 1 Epoch 44 LR 7.94e-05] Train loss 546.985 | Valid NRMSE 0.2484, Pearson 0.417, Score 0.5841 | Patience: 3\n",
      "[Fold 1 Epoch 45 LR 6.55e-05] Train loss 541.651 | Valid NRMSE 0.2759, Pearson 0.394, Score 0.5590 | Patience: 4\n",
      "[Fold 1 Epoch 46 LR 5.00e-05] Train loss 542.881 | Valid NRMSE 0.2579, Pearson 0.416, Score 0.5792 | Patience: 5\n",
      "[Fold 1 Epoch 47 LR 3.45e-05] Train loss 545.882 | Valid NRMSE 0.2486, Pearson 0.423, Score 0.5871 | Patience: 6\n",
      "[Fold 1 Epoch 48 LR 2.06e-05] Train loss 538.254 | Valid NRMSE 0.2527, Pearson 0.428, Score 0.5876 | Patience: 7\n",
      "[Fold 1 Epoch 49 LR 9.55e-06] Train loss 538.228 | Valid NRMSE 0.2497, Pearson 0.427, Score 0.5885 | Patience: 8\n",
      "[Fold 1 Epoch 50 LR 2.45e-06] Train loss 554.910 | Valid NRMSE 0.2502, Pearson 0.426, Score 0.5878 | Patience: 9\n",
      "[Fold 1 Epoch 51 LR 1.00e-04] Train loss 563.821 | Valid NRMSE 0.2579, Pearson 0.435, Score 0.5886 | Patience: 10\n",
      "[Fold 1 Epoch 52 LR 9.76e-05] Train loss 569.166 | Valid NRMSE 0.2551, Pearson 0.409, Score 0.5772 | Patience: 11\n",
      "[Fold 1 Epoch 53 LR 9.05e-05] Train loss 529.867 | Valid NRMSE 0.2631, Pearson 0.407, Score 0.5720 | Patience: 12\n",
      "[Fold 1 Epoch 54 LR 7.94e-05] Train loss 520.460 | Valid NRMSE 0.2538, Pearson 0.397, Score 0.5714 | Patience: 13\n",
      "[Fold 1 Epoch 55 LR 6.55e-05] Train loss 520.922 | Valid NRMSE 0.2499, Pearson 0.425, Score 0.5878 | Patience: 14\n",
      "[Fold 1 Epoch 56 LR 5.00e-05] Train loss 532.427 | Valid NRMSE 0.2497, Pearson 0.415, Score 0.5825 | Patience: 15\n",
      "[Fold 1 Epoch 57 LR 3.45e-05] Train loss 531.601 | Valid NRMSE 0.2561, Pearson 0.415, Score 0.5795 | Patience: 16\n",
      "[Fold 1 Epoch 58 LR 2.06e-05] Train loss 515.829 | Valid NRMSE 0.2520, Pearson 0.418, Score 0.5831 | Patience: 17\n",
      "[Fold 1 Epoch 59 LR 9.55e-06] Train loss 507.201 | Valid NRMSE 0.2519, Pearson 0.422, Score 0.5851 | Patience: 18\n",
      "[Fold 1 Epoch 60 LR 2.45e-06] Train loss 505.400 | Valid NRMSE 0.2526, Pearson 0.424, Score 0.5859 | Patience: 19\n",
      "[Fold 1 Epoch 61 LR 1.00e-04] Train loss 513.531 | Valid NRMSE 0.2667, Pearson 0.407, Score 0.5702 | Patience: 20\n",
      "[Fold 1 Epoch 62 LR 9.76e-05] Train loss 532.504 | Valid NRMSE 0.2548, Pearson 0.412, Score 0.5784 | Patience: 21\n",
      "[Fold 1 Epoch 63 LR 9.05e-05] Train loss 493.432 | Valid NRMSE 0.2751, Pearson 0.409, Score 0.5669 | Patience: 22\n",
      "[Fold 1 Epoch 64 LR 7.94e-05] Train loss 521.975 | Valid NRMSE 0.2565, Pearson 0.428, Score 0.5858 | Patience: 23\n",
      "[Fold 1 Epoch 65 LR 6.55e-05] Train loss 502.848 | Valid NRMSE 0.2674, Pearson 0.411, Score 0.5720 | Patience: 24\n",
      "[Fold 1 Epoch 66 LR 5.00e-05] Train loss 503.916 | Valid NRMSE 0.2559, Pearson 0.418, Score 0.5813 | Patience: 25\n",
      "model saved in ./model_weights/model_2025_1.pt\n",
      "[Fold 1] Best epoch 41 | NRMSE: 0.2466 Pearson: 0.4287 Score: 0.5911\n",
      "\n",
      "\n",
      "========== Fold 2/3 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "차수 계산 완료, degree: tensor([   524,  42900, 115643,  71667,   4566,      1,      1,      0,      0,\n",
      "             0])\n",
      "가중치 로드 완료.\n",
      "사전 학습된 인코더를 동결하지 않습니다 (전체 모델 학습).\n",
      "[Fold 2 Epoch 01 LR 1.00e-04] Train loss 1401.397 | Valid NRMSE 0.3939, Pearson 0.209, Score 0.4076 | Patience: 0\n",
      "[Fold 2 Epoch 02 LR 9.76e-05] Train loss 1218.970 | Valid NRMSE 0.4083, Pearson 0.265, Score 0.4286 | Patience: 0\n",
      "[Fold 2 Epoch 03 LR 9.05e-05] Train loss 1064.577 | Valid NRMSE 0.3541, Pearson 0.319, Score 0.4822 | Patience: 0\n",
      "[Fold 2 Epoch 04 LR 7.94e-05] Train loss 985.344 | Valid NRMSE 0.3556, Pearson 0.353, Score 0.4986 | Patience: 0\n",
      "[Fold 2 Epoch 05 LR 6.55e-05] Train loss 925.280 | Valid NRMSE 0.3140, Pearson 0.368, Score 0.5270 | Patience: 0\n",
      "[Fold 2 Epoch 06 LR 5.00e-05] Train loss 894.295 | Valid NRMSE 0.3055, Pearson 0.377, Score 0.5360 | Patience: 0\n",
      "[Fold 2 Epoch 07 LR 3.45e-05] Train loss 867.300 | Valid NRMSE 0.3064, Pearson 0.394, Score 0.5438 | Patience: 0\n",
      "[Fold 2 Epoch 08 LR 2.06e-05] Train loss 846.779 | Valid NRMSE 0.2817, Pearson 0.402, Score 0.5601 | Patience: 0\n",
      "[Fold 2 Epoch 09 LR 9.55e-06] Train loss 837.728 | Valid NRMSE 0.2832, Pearson 0.405, Score 0.5610 | Patience: 0\n",
      "[Fold 2 Epoch 10 LR 2.45e-06] Train loss 832.382 | Valid NRMSE 0.2820, Pearson 0.406, Score 0.5621 | Patience: 0\n",
      "[Fold 2 Epoch 11 LR 1.00e-04] Train loss 830.271 | Valid NRMSE 0.3173, Pearson 0.395, Score 0.5387 | Patience: 1\n",
      "[Fold 2 Epoch 12 LR 9.76e-05] Train loss 792.034 | Valid NRMSE 0.2604, Pearson 0.399, Score 0.5692 | Patience: 0\n",
      "[Fold 2 Epoch 13 LR 9.05e-05] Train loss 763.354 | Valid NRMSE 0.2647, Pearson 0.428, Score 0.5816 | Patience: 0\n",
      "[Fold 2 Epoch 14 LR 7.94e-05] Train loss 741.792 | Valid NRMSE 0.2678, Pearson 0.428, Score 0.5800 | Patience: 1\n",
      "[Fold 2 Epoch 15 LR 6.55e-05] Train loss 721.521 | Valid NRMSE 0.2752, Pearson 0.429, Score 0.5770 | Patience: 2\n",
      "[Fold 2 Epoch 16 LR 5.00e-05] Train loss 707.977 | Valid NRMSE 0.2532, Pearson 0.426, Score 0.5862 | Patience: 0\n",
      "[Fold 2 Epoch 17 LR 3.45e-05] Train loss 690.785 | Valid NRMSE 0.2547, Pearson 0.438, Score 0.5917 | Patience: 0\n",
      "[Fold 2 Epoch 18 LR 2.06e-05] Train loss 681.350 | Valid NRMSE 0.2568, Pearson 0.442, Score 0.5928 | Patience: 0\n",
      "[Fold 2 Epoch 19 LR 9.55e-06] Train loss 669.338 | Valid NRMSE 0.2523, Pearson 0.438, Score 0.5926 | Patience: 1\n",
      "[Fold 2 Epoch 20 LR 2.45e-06] Train loss 677.288 | Valid NRMSE 0.2538, Pearson 0.437, Score 0.5918 | Patience: 2\n",
      "[Fold 2 Epoch 21 LR 1.00e-04] Train loss 685.051 | Valid NRMSE 0.2848, Pearson 0.428, Score 0.5716 | Patience: 3\n",
      "[Fold 2 Epoch 22 LR 9.76e-05] Train loss 672.197 | Valid NRMSE 0.2708, Pearson 0.411, Score 0.5699 | Patience: 4\n",
      "[Fold 2 Epoch 23 LR 9.05e-05] Train loss 665.282 | Valid NRMSE 0.2632, Pearson 0.421, Score 0.5787 | Patience: 5\n",
      "[Fold 2 Epoch 24 LR 7.94e-05] Train loss 654.662 | Valid NRMSE 0.2496, Pearson 0.439, Score 0.5949 | Patience: 0\n",
      "[Fold 2 Epoch 25 LR 6.55e-05] Train loss 644.220 | Valid NRMSE 0.2596, Pearson 0.446, Score 0.5930 | Patience: 1\n",
      "[Fold 2 Epoch 26 LR 5.00e-05] Train loss 635.831 | Valid NRMSE 0.2504, Pearson 0.448, Score 0.5987 | Patience: 0\n",
      "[Fold 2 Epoch 27 LR 3.45e-05] Train loss 618.416 | Valid NRMSE 0.2506, Pearson 0.455, Score 0.6022 | Patience: 0\n",
      "[Fold 2 Epoch 28 LR 2.06e-05] Train loss 618.514 | Valid NRMSE 0.2479, Pearson 0.455, Score 0.6035 | Patience: 0\n",
      "[Fold 2 Epoch 29 LR 9.55e-06] Train loss 613.262 | Valid NRMSE 0.2529, Pearson 0.456, Score 0.6016 | Patience: 1\n",
      "[Fold 2 Epoch 30 LR 2.45e-06] Train loss 609.438 | Valid NRMSE 0.2494, Pearson 0.453, Score 0.6017 | Patience: 2\n",
      "[Fold 2 Epoch 31 LR 1.00e-04] Train loss 616.578 | Valid NRMSE 0.2527, Pearson 0.423, Score 0.5854 | Patience: 3\n",
      "[Fold 2 Epoch 32 LR 9.76e-05] Train loss 619.927 | Valid NRMSE 0.2553, Pearson 0.441, Score 0.5928 | Patience: 4\n",
      "[Fold 2 Epoch 33 LR 9.05e-05] Train loss 614.373 | Valid NRMSE 0.2499, Pearson 0.451, Score 0.6003 | Patience: 5\n",
      "[Fold 2 Epoch 34 LR 7.94e-05] Train loss 608.908 | Valid NRMSE 0.2486, Pearson 0.462, Score 0.6066 | Patience: 0\n",
      "[Fold 2 Epoch 35 LR 6.55e-05] Train loss 592.953 | Valid NRMSE 0.2584, Pearson 0.454, Score 0.5979 | Patience: 1\n",
      "[Fold 2 Epoch 36 LR 5.00e-05] Train loss 588.920 | Valid NRMSE 0.2495, Pearson 0.458, Score 0.6042 | Patience: 2\n",
      "[Fold 2 Epoch 37 LR 3.45e-05] Train loss 588.584 | Valid NRMSE 0.2479, Pearson 0.464, Score 0.6081 | Patience: 0\n",
      "[Fold 2 Epoch 38 LR 2.06e-05] Train loss 589.665 | Valid NRMSE 0.2484, Pearson 0.467, Score 0.6095 | Patience: 0\n",
      "[Fold 2 Epoch 39 LR 9.55e-06] Train loss 581.280 | Valid NRMSE 0.2483, Pearson 0.467, Score 0.6094 | Patience: 1\n",
      "[Fold 2 Epoch 40 LR 2.45e-06] Train loss 572.601 | Valid NRMSE 0.2485, Pearson 0.469, Score 0.6103 | Patience: 0\n",
      "[Fold 2 Epoch 41 LR 1.00e-04] Train loss 578.302 | Valid NRMSE 0.2500, Pearson 0.458, Score 0.6042 | Patience: 1\n",
      "[Fold 2 Epoch 42 LR 9.76e-05] Train loss 579.243 | Valid NRMSE 0.2571, Pearson 0.465, Score 0.6039 | Patience: 2\n",
      "[Fold 2 Epoch 43 LR 9.05e-05] Train loss 586.376 | Valid NRMSE 0.2514, Pearson 0.477, Score 0.6126 | Patience: 0\n",
      "[Fold 2 Epoch 44 LR 7.94e-05] Train loss 572.421 | Valid NRMSE 0.2521, Pearson 0.460, Score 0.6037 | Patience: 1\n",
      "[Fold 2 Epoch 45 LR 6.55e-05] Train loss 567.348 | Valid NRMSE 0.2507, Pearson 0.463, Score 0.6063 | Patience: 2\n",
      "[Fold 2 Epoch 46 LR 5.00e-05] Train loss 566.865 | Valid NRMSE 0.2513, Pearson 0.456, Score 0.6022 | Patience: 3\n",
      "[Fold 2 Epoch 47 LR 3.45e-05] Train loss 553.907 | Valid NRMSE 0.2496, Pearson 0.465, Score 0.6079 | Patience: 4\n",
      "[Fold 2 Epoch 48 LR 2.06e-05] Train loss 555.901 | Valid NRMSE 0.2507, Pearson 0.465, Score 0.6074 | Patience: 5\n",
      "[Fold 2 Epoch 49 LR 9.55e-06] Train loss 546.099 | Valid NRMSE 0.2514, Pearson 0.461, Score 0.6049 | Patience: 6\n",
      "[Fold 2 Epoch 50 LR 2.45e-06] Train loss 545.164 | Valid NRMSE 0.2505, Pearson 0.465, Score 0.6072 | Patience: 7\n",
      "[Fold 2 Epoch 51 LR 1.00e-04] Train loss 564.132 | Valid NRMSE 0.2521, Pearson 0.469, Score 0.6087 | Patience: 8\n",
      "[Fold 2 Epoch 52 LR 9.76e-05] Train loss 553.254 | Valid NRMSE 0.2659, Pearson 0.464, Score 0.5992 | Patience: 9\n",
      "[Fold 2 Epoch 53 LR 9.05e-05] Train loss 558.678 | Valid NRMSE 0.2532, Pearson 0.446, Score 0.5966 | Patience: 10\n",
      "[Fold 2 Epoch 54 LR 7.94e-05] Train loss 550.777 | Valid NRMSE 0.2512, Pearson 0.462, Score 0.6052 | Patience: 11\n",
      "[Fold 2 Epoch 55 LR 6.55e-05] Train loss 546.505 | Valid NRMSE 0.2554, Pearson 0.456, Score 0.6001 | Patience: 12\n",
      "[Fold 2 Epoch 56 LR 5.00e-05] Train loss 537.710 | Valid NRMSE 0.2523, Pearson 0.468, Score 0.6076 | Patience: 13\n",
      "[Fold 2 Epoch 57 LR 3.45e-05] Train loss 531.676 | Valid NRMSE 0.2534, Pearson 0.455, Score 0.6006 | Patience: 14\n",
      "[Fold 2 Epoch 58 LR 2.06e-05] Train loss 531.693 | Valid NRMSE 0.2506, Pearson 0.471, Score 0.6101 | Patience: 15\n",
      "[Fold 2 Epoch 59 LR 9.55e-06] Train loss 520.960 | Valid NRMSE 0.2517, Pearson 0.465, Score 0.6066 | Patience: 16\n",
      "[Fold 2 Epoch 60 LR 2.45e-06] Train loss 522.516 | Valid NRMSE 0.2514, Pearson 0.467, Score 0.6076 | Patience: 17\n",
      "[Fold 2 Epoch 61 LR 1.00e-04] Train loss 544.436 | Valid NRMSE 0.2528, Pearson 0.453, Score 0.5999 | Patience: 18\n",
      "[Fold 2 Epoch 62 LR 9.76e-05] Train loss 539.836 | Valid NRMSE 0.2584, Pearson 0.438, Score 0.5900 | Patience: 19\n",
      "[Fold 2 Epoch 63 LR 9.05e-05] Train loss 534.908 | Valid NRMSE 0.2533, Pearson 0.459, Score 0.6026 | Patience: 20\n",
      "[Fold 2 Epoch 64 LR 7.94e-05] Train loss 530.305 | Valid NRMSE 0.2520, Pearson 0.480, Score 0.6140 | Patience: 0\n",
      "[Fold 2 Epoch 65 LR 6.55e-05] Train loss 519.554 | Valid NRMSE 0.2638, Pearson 0.466, Score 0.6009 | Patience: 1\n",
      "[Fold 2 Epoch 66 LR 5.00e-05] Train loss 521.828 | Valid NRMSE 0.2548, Pearson 0.462, Score 0.6035 | Patience: 2\n",
      "[Fold 2 Epoch 67 LR 3.45e-05] Train loss 509.973 | Valid NRMSE 0.2533, Pearson 0.470, Score 0.6082 | Patience: 3\n",
      "[Fold 2 Epoch 68 LR 2.06e-05] Train loss 505.646 | Valid NRMSE 0.2542, Pearson 0.461, Score 0.6035 | Patience: 4\n",
      "[Fold 2 Epoch 69 LR 9.55e-06] Train loss 509.893 | Valid NRMSE 0.2528, Pearson 0.466, Score 0.6068 | Patience: 5\n",
      "[Fold 2 Epoch 70 LR 2.45e-06] Train loss 503.917 | Valid NRMSE 0.2533, Pearson 0.464, Score 0.6053 | Patience: 6\n",
      "[Fold 2 Epoch 71 LR 1.00e-04] Train loss 527.429 | Valid NRMSE 0.2609, Pearson 0.449, Score 0.5940 | Patience: 7\n",
      "[Fold 2 Epoch 72 LR 9.76e-05] Train loss 519.891 | Valid NRMSE 0.2689, Pearson 0.440, Score 0.5855 | Patience: 8\n",
      "[Fold 2 Epoch 73 LR 9.05e-05] Train loss 509.436 | Valid NRMSE 0.2554, Pearson 0.461, Score 0.6030 | Patience: 9\n",
      "[Fold 2 Epoch 74 LR 7.94e-05] Train loss 511.352 | Valid NRMSE 0.2557, Pearson 0.452, Score 0.5979 | Patience: 10\n",
      "[Fold 2 Epoch 75 LR 6.55e-05] Train loss 507.249 | Valid NRMSE 0.2565, Pearson 0.460, Score 0.6017 | Patience: 11\n",
      "[Fold 2 Epoch 76 LR 5.00e-05] Train loss 509.735 | Valid NRMSE 0.2554, Pearson 0.454, Score 0.5991 | Patience: 12\n",
      "[Fold 2 Epoch 77 LR 3.45e-05] Train loss 491.798 | Valid NRMSE 0.2592, Pearson 0.444, Score 0.5922 | Patience: 13\n",
      "[Fold 2 Epoch 78 LR 2.06e-05] Train loss 493.357 | Valid NRMSE 0.2551, Pearson 0.462, Score 0.6033 | Patience: 14\n",
      "[Fold 2 Epoch 79 LR 9.55e-06] Train loss 480.913 | Valid NRMSE 0.2571, Pearson 0.456, Score 0.5994 | Patience: 15\n",
      "[Fold 2 Epoch 80 LR 2.45e-06] Train loss 487.293 | Valid NRMSE 0.2569, Pearson 0.456, Score 0.5995 | Patience: 16\n",
      "[Fold 2 Epoch 81 LR 1.00e-04] Train loss 495.571 | Valid NRMSE 0.2586, Pearson 0.456, Score 0.5985 | Patience: 17\n",
      "[Fold 2 Epoch 82 LR 9.76e-05] Train loss 501.361 | Valid NRMSE 0.2590, Pearson 0.445, Score 0.5931 | Patience: 18\n",
      "[Fold 2 Epoch 83 LR 9.05e-05] Train loss 502.085 | Valid NRMSE 0.2543, Pearson 0.471, Score 0.6082 | Patience: 19\n",
      "[Fold 2 Epoch 84 LR 7.94e-05] Train loss 495.788 | Valid NRMSE 0.2568, Pearson 0.461, Score 0.6023 | Patience: 20\n",
      "[Fold 2 Epoch 85 LR 6.55e-05] Train loss 485.402 | Valid NRMSE 0.2599, Pearson 0.441, Score 0.5904 | Patience: 21\n",
      "[Fold 2 Epoch 86 LR 5.00e-05] Train loss 493.889 | Valid NRMSE 0.2568, Pearson 0.458, Score 0.6004 | Patience: 22\n",
      "[Fold 2 Epoch 87 LR 3.45e-05] Train loss 467.494 | Valid NRMSE 0.2619, Pearson 0.449, Score 0.5935 | Patience: 23\n",
      "[Fold 2 Epoch 88 LR 2.06e-05] Train loss 472.365 | Valid NRMSE 0.2576, Pearson 0.453, Score 0.5978 | Patience: 24\n",
      "[Fold 2 Epoch 89 LR 9.55e-06] Train loss 476.317 | Valid NRMSE 0.2569, Pearson 0.458, Score 0.6006 | Patience: 25\n",
      "model saved in ./model_weights/model_2025_2.pt\n",
      "[Fold 2] Best epoch 64 | NRMSE: 0.2520 Pearson: 0.4800 Score: 0.6140\n",
      "\n",
      "\n",
      "========== Fold 3/3 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "차수 계산 완료, degree: tensor([   547,  42890, 116484,  71708,   4559,      0,      1,      0,      0,\n",
      "             0])\n",
      "가중치 로드 완료.\n",
      "사전 학습된 인코더를 동결하지 않습니다 (전체 모델 학습).\n",
      "[Fold 3 Epoch 01 LR 1.00e-04] Train loss 1098.548 | Valid NRMSE 0.3029, Pearson 0.266, Score 0.4813 | Patience: 0\n",
      "[Fold 3 Epoch 02 LR 9.76e-05] Train loss 975.604 | Valid NRMSE 0.2919, Pearson 0.314, Score 0.5111 | Patience: 0\n",
      "[Fold 3 Epoch 03 LR 9.05e-05] Train loss 901.437 | Valid NRMSE 0.2962, Pearson 0.328, Score 0.5160 | Patience: 0\n",
      "[Fold 3 Epoch 04 LR 7.94e-05] Train loss 839.399 | Valid NRMSE 0.2827, Pearson 0.354, Score 0.5356 | Patience: 0\n",
      "[Fold 3 Epoch 05 LR 6.55e-05] Train loss 804.229 | Valid NRMSE 0.2570, Pearson 0.374, Score 0.5583 | Patience: 0\n",
      "[Fold 3 Epoch 06 LR 5.00e-05] Train loss 784.657 | Valid NRMSE 0.2678, Pearson 0.377, Score 0.5547 | Patience: 1\n",
      "[Fold 3 Epoch 07 LR 3.45e-05] Train loss 768.173 | Valid NRMSE 0.2564, Pearson 0.372, Score 0.5577 | Patience: 2\n",
      "[Fold 3 Epoch 08 LR 2.06e-05] Train loss 748.945 | Valid NRMSE 0.2535, Pearson 0.371, Score 0.5588 | Patience: 0\n",
      "[Fold 3 Epoch 09 LR 9.55e-06] Train loss 732.819 | Valid NRMSE 0.2568, Pearson 0.385, Score 0.5639 | Patience: 0\n",
      "[Fold 3 Epoch 10 LR 2.45e-06] Train loss 729.000 | Valid NRMSE 0.2544, Pearson 0.389, Score 0.5673 | Patience: 0\n",
      "[Fold 3 Epoch 11 LR 1.00e-04] Train loss 761.642 | Valid NRMSE 0.2656, Pearson 0.404, Score 0.5694 | Patience: 0\n",
      "[Fold 3 Epoch 12 LR 9.76e-05] Train loss 714.004 | Valid NRMSE 0.2513, Pearson 0.382, Score 0.5653 | Patience: 1\n",
      "[Fold 3 Epoch 13 LR 9.05e-05] Train loss 707.746 | Valid NRMSE 0.2493, Pearson 0.402, Score 0.5765 | Patience: 0\n",
      "[Fold 3 Epoch 14 LR 7.94e-05] Train loss 692.736 | Valid NRMSE 0.2575, Pearson 0.363, Score 0.5527 | Patience: 1\n",
      "[Fold 3 Epoch 15 LR 6.55e-05] Train loss 668.122 | Valid NRMSE 0.2578, Pearson 0.385, Score 0.5637 | Patience: 2\n",
      "[Fold 3 Epoch 16 LR 5.00e-05] Train loss 668.200 | Valid NRMSE 0.2567, Pearson 0.375, Score 0.5589 | Patience: 3\n",
      "[Fold 3 Epoch 17 LR 3.45e-05] Train loss 655.570 | Valid NRMSE 0.2488, Pearson 0.419, Score 0.5850 | Patience: 0\n",
      "[Fold 3 Epoch 18 LR 2.06e-05] Train loss 646.707 | Valid NRMSE 0.2468, Pearson 0.429, Score 0.5909 | Patience: 0\n",
      "[Fold 3 Epoch 19 LR 9.55e-06] Train loss 639.132 | Valid NRMSE 0.2482, Pearson 0.420, Score 0.5860 | Patience: 1\n",
      "[Fold 3 Epoch 20 LR 2.45e-06] Train loss 635.760 | Valid NRMSE 0.2487, Pearson 0.419, Score 0.5851 | Patience: 2\n",
      "[Fold 3 Epoch 21 LR 1.00e-04] Train loss 646.980 | Valid NRMSE 0.2489, Pearson 0.419, Score 0.5853 | Patience: 3\n",
      "[Fold 3 Epoch 22 LR 9.76e-05] Train loss 655.940 | Valid NRMSE 0.2484, Pearson 0.434, Score 0.5927 | Patience: 0\n",
      "[Fold 3 Epoch 23 LR 9.05e-05] Train loss 631.479 | Valid NRMSE 0.2503, Pearson 0.404, Score 0.5768 | Patience: 1\n",
      "[Fold 3 Epoch 24 LR 7.94e-05] Train loss 635.457 | Valid NRMSE 0.2473, Pearson 0.438, Score 0.5953 | Patience: 0\n",
      "[Fold 3 Epoch 25 LR 6.55e-05] Train loss 608.183 | Valid NRMSE 0.2500, Pearson 0.423, Score 0.5864 | Patience: 1\n",
      "[Fold 3 Epoch 26 LR 5.00e-05] Train loss 607.658 | Valid NRMSE 0.2513, Pearson 0.415, Score 0.5816 | Patience: 2\n",
      "[Fold 3 Epoch 27 LR 3.45e-05] Train loss 600.623 | Valid NRMSE 0.2491, Pearson 0.434, Score 0.5924 | Patience: 3\n",
      "[Fold 3 Epoch 28 LR 2.06e-05] Train loss 593.952 | Valid NRMSE 0.2469, Pearson 0.440, Score 0.5966 | Patience: 0\n",
      "[Fold 3 Epoch 29 LR 9.55e-06] Train loss 605.457 | Valid NRMSE 0.2486, Pearson 0.433, Score 0.5920 | Patience: 1\n",
      "[Fold 3 Epoch 30 LR 2.45e-06] Train loss 599.089 | Valid NRMSE 0.2483, Pearson 0.433, Score 0.5924 | Patience: 2\n",
      "[Fold 3 Epoch 31 LR 1.00e-04] Train loss 613.857 | Valid NRMSE 0.2510, Pearson 0.448, Score 0.5984 | Patience: 0\n",
      "[Fold 3 Epoch 32 LR 9.76e-05] Train loss 614.791 | Valid NRMSE 0.2512, Pearson 0.431, Score 0.5899 | Patience: 1\n",
      "[Fold 3 Epoch 33 LR 9.05e-05] Train loss 592.732 | Valid NRMSE 0.2482, Pearson 0.436, Score 0.5940 | Patience: 2\n",
      "[Fold 3 Epoch 34 LR 7.94e-05] Train loss 581.109 | Valid NRMSE 0.2517, Pearson 0.419, Score 0.5838 | Patience: 3\n",
      "[Fold 3 Epoch 35 LR 6.55e-05] Train loss 586.510 | Valid NRMSE 0.2448, Pearson 0.455, Score 0.6049 | Patience: 0\n",
      "[Fold 3 Epoch 36 LR 5.00e-05] Train loss 573.249 | Valid NRMSE 0.2488, Pearson 0.446, Score 0.5988 | Patience: 1\n",
      "[Fold 3 Epoch 37 LR 3.45e-05] Train loss 566.370 | Valid NRMSE 0.2445, Pearson 0.461, Score 0.6081 | Patience: 0\n",
      "[Fold 3 Epoch 38 LR 2.06e-05] Train loss 563.758 | Valid NRMSE 0.2481, Pearson 0.443, Score 0.5976 | Patience: 1\n",
      "[Fold 3 Epoch 39 LR 9.55e-06] Train loss 552.983 | Valid NRMSE 0.2478, Pearson 0.446, Score 0.5991 | Patience: 2\n",
      "[Fold 3 Epoch 40 LR 2.45e-06] Train loss 568.345 | Valid NRMSE 0.2478, Pearson 0.448, Score 0.6002 | Patience: 3\n",
      "[Fold 3 Epoch 41 LR 1.00e-04] Train loss 577.026 | Valid NRMSE 0.2509, Pearson 0.435, Score 0.5920 | Patience: 4\n",
      "[Fold 3 Epoch 42 LR 9.76e-05] Train loss 562.632 | Valid NRMSE 0.2475, Pearson 0.450, Score 0.6015 | Patience: 5\n",
      "[Fold 3 Epoch 43 LR 9.05e-05] Train loss 560.636 | Valid NRMSE 0.2517, Pearson 0.441, Score 0.5944 | Patience: 6\n",
      "[Fold 3 Epoch 44 LR 7.94e-05] Train loss 567.471 | Valid NRMSE 0.2478, Pearson 0.447, Score 0.5994 | Patience: 7\n",
      "[Fold 3 Epoch 45 LR 6.55e-05] Train loss 538.416 | Valid NRMSE 0.2480, Pearson 0.466, Score 0.6092 | Patience: 0\n",
      "[Fold 3 Epoch 46 LR 5.00e-05] Train loss 539.090 | Valid NRMSE 0.2502, Pearson 0.450, Score 0.5999 | Patience: 1\n",
      "[Fold 3 Epoch 47 LR 3.45e-05] Train loss 539.147 | Valid NRMSE 0.2513, Pearson 0.445, Score 0.5968 | Patience: 2\n",
      "[Fold 3 Epoch 48 LR 2.06e-05] Train loss 527.897 | Valid NRMSE 0.2500, Pearson 0.447, Score 0.5985 | Patience: 3\n",
      "[Fold 3 Epoch 49 LR 9.55e-06] Train loss 524.786 | Valid NRMSE 0.2482, Pearson 0.454, Score 0.6031 | Patience: 4\n",
      "[Fold 3 Epoch 50 LR 2.45e-06] Train loss 529.279 | Valid NRMSE 0.2492, Pearson 0.455, Score 0.6031 | Patience: 5\n",
      "[Fold 3 Epoch 51 LR 1.00e-04] Train loss 537.132 | Valid NRMSE 0.2569, Pearson 0.434, Score 0.5883 | Patience: 6\n",
      "[Fold 3 Epoch 52 LR 9.76e-05] Train loss 531.504 | Valid NRMSE 0.2508, Pearson 0.452, Score 0.6008 | Patience: 7\n",
      "[Fold 3 Epoch 53 LR 9.05e-05] Train loss 536.582 | Valid NRMSE 0.2527, Pearson 0.444, Score 0.5955 | Patience: 8\n",
      "[Fold 3 Epoch 54 LR 7.94e-05] Train loss 525.775 | Valid NRMSE 0.2599, Pearson 0.450, Score 0.5949 | Patience: 9\n",
      "[Fold 3 Epoch 55 LR 6.55e-05] Train loss 537.730 | Valid NRMSE 0.2460, Pearson 0.472, Score 0.6132 | Patience: 0\n",
      "[Fold 3 Epoch 56 LR 5.00e-05] Train loss 514.228 | Valid NRMSE 0.2502, Pearson 0.459, Score 0.6042 | Patience: 1\n",
      "[Fold 3 Epoch 57 LR 3.45e-05] Train loss 510.465 | Valid NRMSE 0.2513, Pearson 0.458, Score 0.6034 | Patience: 2\n",
      "[Fold 3 Epoch 58 LR 2.06e-05] Train loss 514.167 | Valid NRMSE 0.2499, Pearson 0.464, Score 0.6070 | Patience: 3\n",
      "[Fold 3 Epoch 59 LR 9.55e-06] Train loss 507.317 | Valid NRMSE 0.2512, Pearson 0.457, Score 0.6029 | Patience: 4\n",
      "[Fold 3 Epoch 60 LR 2.45e-06] Train loss 510.748 | Valid NRMSE 0.2501, Pearson 0.457, Score 0.6036 | Patience: 5\n",
      "[Fold 3 Epoch 61 LR 1.00e-04] Train loss 524.761 | Valid NRMSE 0.2554, Pearson 0.451, Score 0.5980 | Patience: 6\n",
      "[Fold 3 Epoch 62 LR 9.76e-05] Train loss 515.490 | Valid NRMSE 0.2547, Pearson 0.476, Score 0.6107 | Patience: 7\n",
      "[Fold 3 Epoch 63 LR 9.05e-05] Train loss 510.707 | Valid NRMSE 0.2515, Pearson 0.463, Score 0.6056 | Patience: 8\n",
      "[Fold 3 Epoch 64 LR 7.94e-05] Train loss 497.472 | Valid NRMSE 0.2503, Pearson 0.460, Score 0.6047 | Patience: 9\n",
      "[Fold 3 Epoch 65 LR 6.55e-05] Train loss 507.894 | Valid NRMSE 0.2553, Pearson 0.463, Score 0.6040 | Patience: 10\n",
      "[Fold 3 Epoch 66 LR 5.00e-05] Train loss 501.091 | Valid NRMSE 0.2515, Pearson 0.455, Score 0.6016 | Patience: 11\n",
      "[Fold 3 Epoch 67 LR 3.45e-05] Train loss 491.980 | Valid NRMSE 0.2526, Pearson 0.457, Score 0.6021 | Patience: 12\n",
      "[Fold 3 Epoch 68 LR 2.06e-05] Train loss 490.502 | Valid NRMSE 0.2501, Pearson 0.462, Score 0.6060 | Patience: 13\n",
      "[Fold 3 Epoch 69 LR 9.55e-06] Train loss 476.682 | Valid NRMSE 0.2500, Pearson 0.468, Score 0.6088 | Patience: 14\n",
      "[Fold 3 Epoch 70 LR 2.45e-06] Train loss 484.172 | Valid NRMSE 0.2496, Pearson 0.469, Score 0.6097 | Patience: 15\n",
      "[Fold 3 Epoch 71 LR 1.00e-04] Train loss 487.793 | Valid NRMSE 0.2552, Pearson 0.452, Score 0.5983 | Patience: 16\n",
      "[Fold 3 Epoch 72 LR 9.76e-05] Train loss 488.266 | Valid NRMSE 0.2545, Pearson 0.460, Score 0.6027 | Patience: 17\n",
      "[Fold 3 Epoch 73 LR 9.05e-05] Train loss 503.457 | Valid NRMSE 0.2578, Pearson 0.421, Score 0.5817 | Patience: 18\n",
      "[Fold 3 Epoch 74 LR 7.94e-05] Train loss 493.007 | Valid NRMSE 0.2590, Pearson 0.448, Score 0.5946 | Patience: 19\n",
      "[Fold 3 Epoch 75 LR 6.55e-05] Train loss 485.735 | Valid NRMSE 0.2545, Pearson 0.453, Score 0.5992 | Patience: 20\n",
      "[Fold 3 Epoch 76 LR 5.00e-05] Train loss 466.047 | Valid NRMSE 0.2553, Pearson 0.442, Score 0.5932 | Patience: 21\n",
      "[Fold 3 Epoch 77 LR 3.45e-05] Train loss 473.945 | Valid NRMSE 0.2542, Pearson 0.458, Score 0.6021 | Patience: 22\n",
      "[Fold 3 Epoch 78 LR 2.06e-05] Train loss 456.750 | Valid NRMSE 0.2524, Pearson 0.456, Score 0.6019 | Patience: 23\n",
      "[Fold 3 Epoch 79 LR 9.55e-06] Train loss 462.674 | Valid NRMSE 0.2535, Pearson 0.463, Score 0.6049 | Patience: 24\n",
      "[Fold 3 Epoch 80 LR 2.45e-06] Train loss 478.346 | Valid NRMSE 0.2527, Pearson 0.465, Score 0.6062 | Patience: 25\n",
      "model saved in ./model_weights/model_2025_3.pt\n",
      "[Fold 3] Best epoch 55 | NRMSE: 0.2460 Pearson: 0.4724 Score: 0.6132\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:57:46.989762Z",
     "start_time": "2025-07-31T00:57:46.973442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_data = pd.read_csv('../data/test.csv')\n",
    "test_data[\"Canonical_Smiles\"] = test_data[\"Canonical_Smiles\"].map(make_smile_canonical)"
   ],
   "id": "94704a9f55a14e20",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:57:50.162071Z",
     "start_time": "2025-07-31T00:57:47.419718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_dataset = OnTheFlyOGBCompatibleSmilesDataset(\n",
    "    root=\"../test_tmp/MySmilesDataset\",\n",
    "    smiles_list=test_data['Canonical_Smiles'].to_list(),\n",
    "    labels_list=None,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=CFG['BATCH_SIZE'],\n",
    ")\n",
    "\n",
    "val_nrmse_list, val_pearson_list, val_score_list = [], [], []\n",
    "valid_preds, valid_targets = [], []\n",
    "test_preds_list = []\n",
    "\n",
    "# for fold, (train_idx, valid_idx) in enumerate(skf.split(data, data['Inhibition_binary'])):\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(data)):\n",
    "    print(f\"\\n\\n===== Fold {fold+1}/{N_SPLITS} =====\")\n",
    "\n",
    "    fold_train = data.iloc[train_idx].copy().reset_index(drop=True)\n",
    "    fold_valid = data.iloc[valid_idx].copy().reset_index(drop=True)\n",
    "\n",
    "    train_dataset = OnTheFlyOGBCompatibleSmilesDataset(\n",
    "        root=\"../train_tmp/MySmilesDataset\",\n",
    "        smiles_list=fold_train['Canonical_Smiles'].to_list(),\n",
    "        labels_list=fold_train['Inhibition'].to_list(),\n",
    "    )\n",
    "    valid_dataset = OnTheFlyOGBCompatibleSmilesDataset(\n",
    "        root=\"../valid_tmp/MySmilesDataset\",\n",
    "        smiles_list=fold_valid['Canonical_Smiles'].to_list(),\n",
    "        labels_list=fold_valid['Inhibition'].to_list(),\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=CFG['BATCH_SIZE'],\n",
    "    )\n",
    "\n",
    "    INPUT_DIM = train_dataset.num_node_features # 9\n",
    "    EDGE_DIM = train_dataset.num_edge_features # 3\n",
    "    HIDDEN_DIM = 128\n",
    "    NUM_LAYERS = 5\n",
    "    deg = cal_degree(train_dataset)\n",
    "\n",
    "    model = pna4finetuning(\n",
    "        input_dim=INPUT_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        deg=deg.to(DEVICE),\n",
    "        edge_dim=EDGE_DIM,\n",
    "        # model_path='model_weights/pna_128_6_epoch30.pt',\n",
    "        model_path='model_weights/pna_128_5_epoch30.pt',\n",
    "        freeze_encoder=False,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    seed = CFG[\"SEED\"]\n",
    "    model_path = f'./model_weights/model_{seed}_{fold+1}.pt'\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    model.eval()\n",
    "\n",
    "    if CFG['USE_AMP']:\n",
    "        scaler = torch.amp.GradScaler()\n",
    "        use_amp = True\n",
    "    else:\n",
    "        use_amp = False\n",
    "\n",
    "    nrmse, pearson, score = valid_fn(model, valid_loader, is_tta=False)\n",
    "    print(f\"[Fold {fold+1}] No TTA | NRMSE: {nrmse:.4f}, Pearson: {pearson:.4f}, Score: {score:.4f}\")\n",
    "\n",
    "    # nrmse, pearson, score = valid_fn(model, valid_loader, is_tta=True)\n",
    "    # print(f\"[Fold {fold+1}] is TTA | NRMSE: {nrmse:.4f}, Pearson: {pearson:.4f}, Score: {score:.4f}\")\n",
    "    #\n",
    "    # mean_score, std_score = evaluate_with_bootstrap(model, valid_loader, is_tta=False, n_repeats=50, sample_ratio=0.1)\n",
    "    # print(f\"[Fold {fold+1}] no TTA | Mean Score: {mean_score:.4f}, std: {std_score:.4f}\")\n",
    "    #\n",
    "    # mean_score, std_score = evaluate_with_bootstrap(model, valid_loader, is_tta=True, n_repeats=50, sample_ratio=0.1)\n",
    "    # print(f\"[Fold {fold+1}] is TTA | Mean Score: {mean_score:.4f}, std: {std_score:.4f}\")\n",
    "\n",
    "    val_nrmse_list.append(nrmse)\n",
    "    val_pearson_list.append(pearson)\n",
    "    val_score_list.append(score)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            batch = batch.to(DEVICE)\n",
    "\n",
    "            pred = model(batch).squeeze(-1)      # (batch,) shape\n",
    "\n",
    "            target = batch.y.squeeze(-1)         # (batch,)\n",
    "            valid_preds += pred.cpu().tolist()\n",
    "            valid_targets += target.cpu().tolist()\n",
    "\n",
    "            nrmse = normalized_rmse(target.cpu().tolist(), pred.cpu().tolist())\n",
    "\n",
    "    ## Test 데이터 예측\n",
    "    predictions = predict(model, test_loader, is_tta=False)\n",
    "    test_preds_list.append(predictions)\n",
    "\n",
    "avg_nrmse = np.mean(val_nrmse_list, axis=0)\n",
    "avg_pearson = np.mean(val_pearson_list, axis=0)\n",
    "avg_score = np.mean(val_score_list, axis=0)\n",
    "\n",
    "seed = CFG['SEED']\n",
    "total_score = competition_score(valid_targets, valid_preds)\n",
    "print(f'[Seed {seed}] Average NRMSE: {avg_nrmse:.4f} Pearson: {avg_pearson:.4f} Score: {avg_score:.4f} | '\n",
    "      f'Total Score: {total_score:.4f}')"
   ],
   "id": "915a9f4a74b7310",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===== Fold 1/3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n",
      "Processing...\n",
      "Done!\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "차수 계산 완료, degree: tensor([   37,  4814, 13145,  8644,   523,     0,     0,     0,     0,     0])\n",
      "가중치 로드 완료.\n",
      "사전 학습된 인코더를 동결하지 않습니다 (전체 모델 학습).\n",
      "[Fold 1] No TTA | NRMSE: 0.2466, Pearson: 0.4287, Score: 0.5911\n",
      "\n",
      "\n",
      "===== Fold 2/3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "차수 계산 완료, degree: tensor([   35,  4855, 13067,  8811,   485,     0,     0,     0,     0,     0])\n",
      "가중치 로드 완료.\n",
      "사전 학습된 인코더를 동결하지 않습니다 (전체 모델 학습).\n",
      "[Fold 2] No TTA | NRMSE: 0.2520, Pearson: 0.4800, Score: 0.6140\n",
      "\n",
      "\n",
      "===== Fold 3/3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "차수 계산 완료, degree: tensor([   36,  4763, 13120,  8597,   500,     0,     0,     0,     0,     0])\n",
      "가중치 로드 완료.\n",
      "사전 학습된 인코더를 동결하지 않습니다 (전체 모델 학습).\n",
      "[Fold 3] No TTA | NRMSE: 0.2460, Pearson: 0.4724, Score: 0.6132\n",
      "[Seed 2025] Average NRMSE: 0.2482 Pearson: 0.4604 Score: 0.6061 | Total Score: 0.6084\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- layernorm + batchnorm: Score: 0.6161 | Total Score: 0.6197\n",
    "- batchnorm: Score: 0.6223 | Total Score: 0.6228"
   ],
   "id": "31ac627fcb3e644d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:57:57.877509Z",
     "start_time": "2025-07-31T00:57:57.873048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "submission = pd.read_csv('../data/sample_submission.csv')\n",
    "submission['Inhibition'] = np.nan\n",
    "submission['Inhibition'] = np.mean(test_preds_list, axis=0)\n",
    "\n",
    "seed = CFG['SEED']\n",
    "save_path_submit = f'../Submission/pna_sigmoid_seed{seed}.csv'\n",
    "submission.to_csv(save_path_submit, index=False)\n",
    "print(f'save submission in {save_path_submit}')"
   ],
   "id": "aa99bd9965193de4",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:58:00.781146Z",
     "start_time": "2025-07-31T00:58:00.776103Z"
    }
   },
   "cell_type": "code",
   "source": "submission",
   "id": "b9b35de51215e9e1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          ID  Inhibition\n",
       "0   TEST_000   54.837250\n",
       "1   TEST_001   66.812386\n",
       "2   TEST_002   34.733204\n",
       "3   TEST_003   25.802750\n",
       "4   TEST_004   19.682375\n",
       "..       ...         ...\n",
       "95  TEST_095   25.447227\n",
       "96  TEST_096   62.866436\n",
       "97  TEST_097   59.519512\n",
       "98  TEST_098   95.576195\n",
       "99  TEST_099   34.404484\n",
       "\n",
       "[100 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Inhibition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000</td>\n",
       "      <td>54.837250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>66.812386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>34.733204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_003</td>\n",
       "      <td>25.802750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_004</td>\n",
       "      <td>19.682375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>TEST_095</td>\n",
       "      <td>25.447227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>TEST_096</td>\n",
       "      <td>62.866436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>TEST_097</td>\n",
       "      <td>59.519512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>TEST_098</td>\n",
       "      <td>95.576195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>TEST_099</td>\n",
       "      <td>34.404484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:58:35.516755Z",
     "start_time": "2025-07-31T00:58:35.511139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "seed_list = [42, 2025]\n",
    "submission_list = []\n",
    "\n",
    "for seed in seed_list:\n",
    "    seed_sub = pd.read_csv(f'../Submission/pna_sigmoid_seed{seed}.csv')\n",
    "    submission_list.append(seed_sub['Inhibition'])\n",
    "\n",
    "submission = pd.read_csv('../data/sample_submission.csv')\n",
    "submission['Inhibition'] = np.nan\n",
    "submission['Inhibition'] = np.mean(submission_list, axis=0)\n",
    "\n",
    "save_path_submit = f'../Submission/pna_sigmoid_seed_ensemble.csv'\n",
    "submission.to_csv(save_path_submit, index=False)\n",
    "print(f'save submission in {save_path_submit}')"
   ],
   "id": "4398ba9f0f6cf8a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save submission in ../Submission/pna_sigmoid_seed_ensemble.csv\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T00:58:35.930539Z",
     "start_time": "2025-07-31T00:58:35.926346Z"
    }
   },
   "cell_type": "code",
   "source": "submission",
   "id": "c49473a7b514410",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          ID  Inhibition\n",
       "0   TEST_000   54.925894\n",
       "1   TEST_001   62.727604\n",
       "2   TEST_002   38.337173\n",
       "3   TEST_003   16.572712\n",
       "4   TEST_004   17.799762\n",
       "..       ...         ...\n",
       "95  TEST_095   26.743040\n",
       "96  TEST_096   54.501399\n",
       "97  TEST_097   60.977345\n",
       "98  TEST_098   93.625410\n",
       "99  TEST_099   28.646995\n",
       "\n",
       "[100 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Inhibition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000</td>\n",
       "      <td>54.925894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>62.727604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>38.337173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_003</td>\n",
       "      <td>16.572712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_004</td>\n",
       "      <td>17.799762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>TEST_095</td>\n",
       "      <td>26.743040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>TEST_096</td>\n",
       "      <td>54.501399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>TEST_097</td>\n",
       "      <td>60.977345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>TEST_098</td>\n",
       "      <td>93.625410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>TEST_099</td>\n",
       "      <td>28.646995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "105897ef0a05890c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
